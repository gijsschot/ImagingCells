Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Xu1996,
abstract = {We build up the mathematical connection between the “Expectation-Maximization” (EM) algorithm and gradient-based approaches for maximum likelihood learning of finite gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P, and we provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the effect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of gaussian mixture models.},
author = {Xu, Lei and Jordan, Michael I},
doi = {10.1162/neco.1996.8.1.129},
issn = {0899-7667},
journal = {Neural Computation},
number = {1},
pages = {129--151},
title = {{On Convergence Properties of the EM Algorithm for Gaussian Mixtures}},
volume = {8},
year = {1996}
}
